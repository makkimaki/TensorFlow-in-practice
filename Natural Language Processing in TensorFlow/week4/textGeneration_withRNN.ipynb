{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.3 64-bit ('base': conda)",
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "398dc28c06ad810e77de546bbdfa897a6ee0b83e59a5207339dda01a7843e01d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation by RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Andrej Karpathy \n",
    "\"The Unreasonable Effectiveness of Recurrent Neural Networks\"\n",
    "\n",
    "QUEENE:\n",
    "I had thought thou hadst a Roman; for the oracle,\n",
    "Thus by All bids the man against the word,\n",
    "Which are so weak of care, by old care done;\n",
    "Your children were in your holy love,\n",
    "And the precipitation through the bleeding throne.\n",
    "\n",
    "BISHOP OF ELY:\n",
    "Marry, and will, my lord, to weep in such a one were prettiest;\n",
    "Yet now I was adopted heir\n",
    "Of the world's lamentable day,\n",
    "To watch the next way with his father with his face?\n",
    "\n",
    "ESCALUS:\n",
    "The cause why then we are all resolved more sons.\n",
    "\n",
    "VOLUMNIA:\n",
    "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
    "And love and pale as any will to that word.\n",
    "\n",
    "QUEEN ELIZABETH:\n",
    "But how long have I heard the soul for this world,\n",
    "And show his hands of life be proved to stand.\n",
    "\n",
    "PETRUCHIO:\n",
    "I say he look'd on, if I must be content\n",
    "To stay him from the fatal of our country's bliss.\n",
    "His lordship pluck'd from this sentence then for prey,\n",
    "And then let us twain, being the moon,\n",
    "were she such a case as fills m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2.0.0\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import numpy as np\n",
    "import os \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n1122304/1115394 [==============================] - 1s 1us/step\n"
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file(\"shakespeare.txt\", 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Length of text: 1115394 characters\n"
    }
   ],
   "source": [
    "# after loading data, decode data for compatibility with python2\n",
    "text = open(path_to_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
    "# the number of characters equals to text length\n",
    "print(\"Length of text: {} characters\".format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\n"
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{' ', 'c', 'F', 'E', 'I', 'R', 'U', 'T', '\\n', 'i', 't', 'X', ';', 'd', 'h', 'Q', 'y', 'z', 'm', 'g', 'B', 'j', 'a', 'l', 'w', 'P', 'u', 'r', 'n', 'A', 'S', 'W', 'K', '3', 'D', 'M', \"'\", 'O', 'o', 'G', 'e', 'k', '&', '$', 'Z', 's', 'x', ':', 'v', '?', 'H', '.', 'b', 'q', '-', 'f', 'L', 'J', 'p', 'Y', '!', 'C', 'V', 'N', ','}\n['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n65 unique characters\n"
    }
   ],
   "source": [
    "#unique numbers of characters\n",
    "factor_vocab = set(text)\n",
    "vocab = sorted(factor_vocab)\n",
    "\n",
    "print(factor_vocab)\n",
    "print(vocab)\n",
    "print(\"{} unique characters\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the text\n",
    "### vectorize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "***************** char2idx *****************\n{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n***************** idx2char *****************\n['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n***************** text_as_int *****************\n[18 47 56 ... 45  8  0]\n"
    }
   ],
   "source": [
    "# make correspondace table of each character and its index\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "print(\"***************** char2idx *****************\")\n",
    "print(char2idx)\n",
    "print(\"***************** idx2char *****************\")\n",
    "print(idx2char)\n",
    "# print(text)\n",
    "print(\"***************** text_as_int *****************\")\n",
    "print(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{\n '\\n':   0,\n ' ' :   1,\n '!' :   2,\n '$' :   3,\n '&' :   4,\n \"'\" :   5,\n ',' :   6,\n '-' :   7,\n '.' :   8,\n '3' :   9,\n ':' :  10,\n ';' :  11,\n '?' :  12,\n 'A' :  13,\n 'B' :  14,\n 'C' :  15,\n 'D' :  16,\n 'E' :  17,\n 'F' :  18,\n 'G' :  19,\n ...\n\n"
    }
   ],
   "source": [
    "print(\"{\")\n",
    "for char, _ in zip(char2idx, range(20)):\n",
    "    print(\" {:4s}: {:3d},\".format(repr(char), char2idx[char]))\n",
    "\n",
    "print(\" ...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "'First Citizen' ----- characters mapped to in  -----> [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
    }
   ],
   "source": [
    "print(\"{} ----- characters mapped to in  -----> {}\".format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction task\n",
    "Given some characters or their sequences, what is the most likely next letter? \n",
    "\n",
    "This is the task you wanto to train your model to do. The input to the model is a string of characters, and we train the model to make predictions about the output, the next character at each point in time.\n",
    "\n",
    "## Create a training sample and target\n",
    "We then split the text into sample sequences. Each input sequence contains seq_length characters from the original text.\n",
    "\n",
    "For each input sequence, the corresponding target contains the same length of text, but shifted to the right by one character.\n",
    "\n",
    "Therefore, the text is split into seq_length+1 chunks. For example, suppose seq_length is 4 and the text is \"Hello\". The input sequence would be \"Hell\" and the target sequence would be \"Hello\".\n",
    "\n",
    "To do this, we first convert the text vector into a sequence of character indexes using the tf.data.Dataset.from_tensor_slices function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "11043\n<TensorSliceDataset shapes: (), types: tf.int64>\nF\ni\nr\ns\nt\n***********************************\n18\ntf.Tensor(18, shape=(), dtype=int64)\n47\ntf.Tensor(47, shape=(), dtype=int64)\n56\ntf.Tensor(56, shape=(), dtype=int64)\n57\ntf.Tensor(57, shape=(), dtype=int64)\n58\ntf.Tensor(58, shape=(), dtype=int64)\n"
    }
   ],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "print(examples_per_epoch)\n",
    "\n",
    "# create training sample and target\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "print(char_dataset)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])\n",
    "\n",
    "print(\"***********************************\")\n",
    "for i in char_dataset.take(5):\n",
    "    print(i.numpy())\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'TensorSliceDataset' object has no attribute 'as_numpy_iterator'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-48955165fd5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TensorSliceDataset' object has no attribute 'as_numpy_iterator'"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "for element in dataset.as_numpy_iterator():\n",
    "  print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(\"\".join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output the first sample input and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\nTarget data:  'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input data: \", repr(\"\".join(idx2char[input_example.numpy()])))\n",
    "    print(\"Target data: \", repr(\"\".join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Step    0\n input: 18 ('F')\n expected output: 47 ('i')\nStep    1\n input: 47 ('i')\n expected output: 56 ('r')\nStep    2\n input: 56 ('r')\n expected output: 57 ('s')\nStep    3\n input: 57 ('s')\n expected output: 58 ('t')\nStep    4\n input: 58 ('t')\n expected output: 1 (' ')\n"
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\" input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\" expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# BUFFER SIZE for shuffling a dataset\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3']"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units= 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer=\"glorot_uniform\"),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'tensorflow.python.data.ops.dataset_ops.TakeDataset'>\n"
    }
   ],
   "source": [
    "print(type(dataset.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_3 (Embedding)      (64, None, 256)           16640     \n_________________________________________________________________\ngru (GRU)                    (64, None, 1024)          3938304   \n_________________________________________________________________\ndense (Dense)                (64, None, 65)            66625     \n=================================================================\nTotal params: 4,021,569\nTrainable params: 4,021,569\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([46, 61, 45, 11, 41, 46, 62, 41, 47, 41, 14, 53,  5, 61, 24, 27, 27,\n       50, 48, 38, 54,  1, 60, 36, 52, 41, 51, 60,  3, 38,  7, 44,  8, 14,\n       25, 53,  9, 56, 31, 21, 31, 10, 57,  2, 26, 18, 37, 12, 10, 43, 25,\n       55, 37,  5, 50, 61, 42, 45, 58, 41, 51, 59, 56, 25, 21, 27, 15, 21,\n       12,  4, 42, 47, 48, 58, 12,  2, 49, 56, 21, 24,  5, 20, 50, 36, 38,\n       53, 56, 39, 21, 51, 64, 49, 31,  5, 64, 47, 10, 15, 16, 19])"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Input: \n 'ar this: mistake me not; no life,\\nI prize it not a straw, but for mine honour,\\nWhich I would free, i'\n\nNext Char Predictions: \n \"hwg;chxcicBo'wLOOljZp vXncmv$Z-f.BMo3rSIS:s!NFY?:eMqY'lwdgtcmurMIOCI?&dijt?!krIL'HlXZoraImzkS'zi:CDG\"\n"
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['a' 'r' ' ' 't' 'h' 'i' 's' ':' ' ' 'm' 'i' 's' 't' 'a' 'k' 'e' ' ' 'm'\n 'e' ' ' 'n' 'o' 't' ';' ' ' 'n' 'o' ' ' 'l' 'i' 'f' 'e' ',' '\\n' 'I' ' '\n 'p' 'r' 'i' 'z' 'e' ' ' 'i' 't' ' ' 'n' 'o' 't' ' ' 'a' ' ' 's' 't' 'r'\n 'a' 'w' ',' ' ' 'b' 'u' 't' ' ' 'f' 'o' 'r' ' ' 'm' 'i' 'n' 'e' ' ' 'h'\n 'o' 'n' 'o' 'u' 'r' ',' '\\n' 'W' 'h' 'i' 'c' 'h' ' ' 'I' ' ' 'w' 'o' 'u'\n 'l' 'd' ' ' 'f' 'r' 'e' 'e' ',' ' ' 'i']\n['h' 'w' 'g' ';' 'c' 'h' 'x' 'c' 'i' 'c' 'B' 'o' \"'\" 'w' 'L' 'O' 'O' 'l'\n 'j' 'Z' 'p' ' ' 'v' 'X' 'n' 'c' 'm' 'v' '$' 'Z' '-' 'f' '.' 'B' 'M' 'o'\n '3' 'r' 'S' 'I' 'S' ':' 's' '!' 'N' 'F' 'Y' '?' ':' 'e' 'M' 'q' 'Y' \"'\"\n 'l' 'w' 'd' 'g' 't' 'c' 'm' 'u' 'r' 'M' 'I' 'O' 'C' 'I' '?' '&' 'd' 'i'\n 'j' 't' '?' '!' 'k' 'r' 'I' 'L' \"'\" 'H' 'l' 'X' 'Z' 'o' 'r' 'a' 'I' 'm'\n 'z' 'k' 'S' \"'\" 'z' 'i' ':' 'C' 'D' 'G']\n"
    }
   ],
   "source": [
    "print(idx2char[input_example_batch[0]])\n",
    "print(idx2char[sampled_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\nscalar_loss:       4.1732717\n"
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"./training_checkpoints\"\n",
    "# name of checkpoint file\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### do training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/10\n172/172 [==============================] - 1049s 6s/step - loss: 2.6507\nEpoch 2/10\n172/172 [==============================] - 807s 5s/step - loss: 1.9499\nEpoch 3/10\n172/172 [==============================] - 1718s 10s/step - loss: 1.6863\nEpoch 4/10\n172/172 [==============================] - 826s 5s/step - loss: 1.5388\nEpoch 5/10\n172/172 [==============================] - 2990s 17s/step - loss: 1.4508\nEpoch 6/10\n172/172 [==============================] - 924s 5s/step - loss: 1.3906\nEpoch 7/10\n172/172 [==============================] - 1062s 6s/step - loss: 1.3443\nEpoch 8/10\n172/172 [==============================] - 1045s 6s/step - loss: 1.3049\nEpoch 9/10\n172/172 [==============================] - 917s 5s/step - loss: 1.2687\nEpoch 10/10\n172/172 [==============================] - 818s 5s/step - loss: 1.2360\n"
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'./training_checkpoints/ckpt_10'"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_4 (Embedding)      (1, None, 256)            16640     \n_________________________________________________________________\ngru_1 (GRU)                  (1, None, 1024)           3938304   \n_________________________________________________________________\ndense_1 (Dense)              (1, None, 65)             66625     \n=================================================================\nTotal params: 4,021,569\nTrainable params: 4,021,569\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "checkpoint                   ckpt_5.data-00000-of-00001\nckpt_1.data-00000-of-00001   ckpt_5.index\nckpt_1.index                 ckpt_6.data-00000-of-00001\nckpt_10.data-00000-of-00001  ckpt_6.index\nckpt_10.index                ckpt_7.data-00000-of-00001\nckpt_2.data-00000-of-00001   ckpt_7.index\nckpt_2.index                 ckpt_8.data-00000-of-00001\nckpt_3.data-00000-of-00001   ckpt_8.index\nckpt_3.index                 ckpt_9.data-00000-of-00001\nckpt_4.data-00000-of-00001   ckpt_9.index\nckpt_4.index\n"
    }
   ],
   "source": [
    "ls {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'./training_checkpoints/ckpt_10'"
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"./checkpoints/my_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction loop\n",
    "* feed a character series, initialize the state of RNN, then set the number of characters generated\n",
    "* Earn the prediction distribution of next character using start characters and the RNN state.\n",
    "* Using category distribution, calculate the index of predicted characters. And feed them into the next model.\n",
    "* These returned RNN states are fed back into the next RNN model so that this would have much more contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # evaluatioin step\n",
    "    # generation character number\n",
    "    num_generate = 1000\n",
    "\n",
    "    # convert the starting characters into integers(vectorize)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    # low 'temperature' would likely bring predictable texts\n",
    "    # high \"temperature\" would likely bring unpredictable texts.\n",
    "    # find optimal conditins through some experiments\n",
    "    temperature = 1.0\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # delete the dim of batch\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # predict the retuned character using categorical distribution\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # pass the last hidden state and predicted characters to model as an input\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    \n",
    "    return (start_string + \"\".join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ROMEO: these have us banished in\nbeing an intelligent for your\nPetruchio of their causes by the charity.\n\nPOLIXENES:\nTrie's good, and am\nI think, she's donat oup the darkless croud,\nAnd there they aspect some other-heart.\n\nCALIABENH:\nUpriest winding on him--as this so divers a heavenly right.\n\nANGELO:\nYou make your gracious bowels--look'd for my i'nt,\nCourts up it daughter'd to thee.\n\nJONTES:\nBring these another, his departered\nhath tripp'd not foundance, madam,\nThou hast not break our scorn his fatach, nay, my lord,\nYour lifes of Follow.\n\nSereft.\n\nSeeve have the bear the man.\nShase not the both of me, and more de with the morning,\nAnd sure I shore nothing heavens that late not have.\n\nCATEBBY:\nWhy, low abward master?\n\nGREMIO:\nTake not by notle last.\n\nCOMINIUS:\nYou have better from the first\nAnd met all the sound hot proud tond men open when you bid Goor labe\ntheir terrant with me with thy looks a\njoy, but as\nyou have not chedities, that whose advise\nby humbishness traniou-daring frown: where \n"
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}