{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Course 1 - Part 6 - Lesson 2 - Notebook.ipynb のコピー",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rX8mhOLljYeM"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "BZSlp3DAjdYf",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R6gHiH-I7uFa"
      },
      "source": [
        "#Improving Computer Vision Accuracy using Convolutions\n",
        "\n",
        "In the previous lessons you saw how to do fashion recognition using a Deep Neural Network (DNN) containing three layers -- the input layer (in the shape of the data), the output layer (in the shape of the desired output) and a hidden layer. You experimented with the impact of different sizes of hidden layer, number of training epochs etc on the final accuracy.\n",
        "\n",
        "For convenience, here's the entire code again. Run it and take a note of the test accuracy that is printed out at the end. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xcsRtq9OLorS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "bbf08e9d-dd8c-4f0b-a423-45bf606b1d79"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images / 255.0\n",
        "test_images=test_images / 255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4943 - accuracy: 0.8256\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3719 - accuracy: 0.8655\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3343 - accuracy: 0.8781\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3103 - accuracy: 0.8848\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2914 - accuracy: 0.8923\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3688 - accuracy: 0.8692\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zldEXSsF8Noz"
      },
      "source": [
        "Your accuracy is probably about 89% on training and 87% on validation...not bad...But how do you make that even better? One way is to use something called Convolutions. I'm not going to details on Convolutions here, but the ultimate concept is that they narrow down the content of the image to focus on specific, distinct, details. \n",
        "\n",
        "If you've ever done image processing using a filter (like this: https://en.wikipedia.org/wiki/Kernel_(image_processing)) then convolutions will look very familiar.\n",
        "\n",
        "In short, you take an array (usually 3x3 or 5x5) and pass it over the image. By changing the underlying pixels based on the formula within that matrix, you can do things like edge detection. So, for example, if you look at the above link, you'll see a 3x3 that is defined for edge detection where the middle cell is 8, and all of its neighbors are -1. In this case, for each pixel, you would multiply its value by 8, then subtract the value of each neighbor. Do this for every pixel, and you'll end up with a new image that has the edges enhanced.\n",
        "\n",
        "This is perfect for computer vision, because often it's features that can get highlighted like this that distinguish one item for another, and the amount of information needed is then much less...because you'll just train on the highlighted features.\n",
        "\n",
        "That's the concept of Convolutional Neural Networks. Add some layers to do convolution before you have the dense layers, and then the information going to the dense layers is more focussed, and possibly more accurate.\n",
        "\n",
        "Run the below code -- this is the same neural network as earlier, but this time with Convolutional layers added first. It will take longer, but look at the impact on the accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C0tFgT1MMKi6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "fa3bb1db-5cd2-4992-96b7-cd1027c56562"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss = model.evaluate(test_images, test_labels)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               204928    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 243,786\n",
            "Trainable params: 243,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4380 - accuracy: 0.8418\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2962 - accuracy: 0.8908\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2510 - accuracy: 0.9069\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2212 - accuracy: 0.9166\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1925 - accuracy: 0.9283\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.2569 - accuracy: 0.9107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PC0QAolxiPtx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "97e71a7e-4fd9-4768-9ca1-b9578c69be5b"
      },
      "source": [
        "%%time\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(training_images, training_labels, epochs=20)\n",
        "test_loss = model.evaluate(test_images, test_labels)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 128)               204928    \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 243,786\n",
            "Trainable params: 243,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4427 - accuracy: 0.8391\n",
            "Epoch 2/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2939 - accuracy: 0.8925\n",
            "Epoch 3/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2499 - accuracy: 0.9079\n",
            "Epoch 4/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2166 - accuracy: 0.9194\n",
            "Epoch 5/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1893 - accuracy: 0.9294\n",
            "Epoch 6/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1678 - accuracy: 0.9371\n",
            "Epoch 7/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1478 - accuracy: 0.9442\n",
            "Epoch 8/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1280 - accuracy: 0.9526\n",
            "Epoch 9/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1134 - accuracy: 0.9576\n",
            "Epoch 10/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0977 - accuracy: 0.9631\n",
            "Epoch 11/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0852 - accuracy: 0.9679\n",
            "Epoch 12/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0773 - accuracy: 0.9701\n",
            "Epoch 13/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0688 - accuracy: 0.9742\n",
            "Epoch 14/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0620 - accuracy: 0.9768\n",
            "Epoch 15/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0566 - accuracy: 0.9783\n",
            "Epoch 16/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0491 - accuracy: 0.9820\n",
            "Epoch 17/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0450 - accuracy: 0.9837\n",
            "Epoch 18/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0448 - accuracy: 0.9833\n",
            "Epoch 19/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0396 - accuracy: 0.9858\n",
            "Epoch 20/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0363 - accuracy: 0.9863\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.5210 - accuracy: 0.9090\n",
            "CPU times: user 2min 4s, sys: 13.3 s, total: 2min 17s\n",
            "Wall time: 1min 59s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uRLfZ0jt-fQI"
      },
      "source": [
        "It's likely gone up to about 93% on the training data and 91% on the validation data. \n",
        "\n",
        "That's significant, and a step in the right direction!\n",
        "\n",
        "Try running it for more epochs -- say about 20, and explore the results! But while the results might seem really good, the validation results may actually go down, due to something called 'overfitting' which will be discussed later. \n",
        "\n",
        "(In a nutshell, 'overfitting' occurs when the network learns the data from the training set really well, but it's too specialised to only that data, and as a result is less effective at seeing *other* data. For example, if all your life you only saw red shoes, then when you see a red shoe you would be very good at identifying it, but blue suade shoes might confuse you...and you know you should never mess with my blue suede shoes.)\n",
        "\n",
        "Then, look at the code again, and see, step by step how the Convolutions were built:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RaLX5cgI_JDb"
      },
      "source": [
        "Step 1 is to gather the data. You'll notice that there's a bit of a change here in that the training data needed to be reshaped. That's because the first convolution expects a single tensor containing everything, so instead of 60,000 28x28x1 items in a list, we have a single 4D list that is 60,000x28x28x1, and the same for the test images. If you don't do this, you'll get an error when training as the Convolutions do not recognize the shape. \n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SS_W_INc_kJQ"
      },
      "source": [
        "Next is to define your model. Now instead of the input layer at the top, you're going to add a Convolution. The parameters are:\n",
        "\n",
        "1. The number of convolutions you want to generate. Purely arbitrary, but good to start with something in the order of 32\n",
        "2. The size of the Convolution, in this case a 3x3 grid\n",
        "3. The activation function to use -- in this case we'll use relu, which you might recall is the equivalent of returning x when x>0, else returning 0\n",
        "4. In the first layer, the shape of the input data.\n",
        "\n",
        "You'll follow the Convolution with a MaxPooling layer which is then designed to compress the image, while maintaining the content of the features that were highlighted by the convlution. By specifying (2,2) for the MaxPooling, the effect is to quarter the size of the image. Without going into too much detail here, the idea is that it creates a 2x2 array of pixels, and picks the biggest one, thus turning 4 pixels into 1. It repeats this across the image, and in so doing halves the number of horizontal, and halves the number of vertical pixels, effectively reducing the image by 25%.\n",
        "\n",
        "You can call model.summary() to see the size and shape of the network, and you'll notice that after every MaxPooling layer, the image size is reduced in this way. \n",
        "\n",
        "\n",
        "```\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RMorM6daADjA"
      },
      "source": [
        "Add another convolution\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b1-x-kZF4_tC"
      },
      "source": [
        "Now flatten the output. After this you'll just have the same DNN structure as the non convolutional version\n",
        "\n",
        "```\n",
        "  tf.keras.layers.Flatten(),\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qPtqR23uASjX"
      },
      "source": [
        "The same 128 dense layers, and 10 output layers as in the pre-convolution example:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C0GSsjUhAaSj"
      },
      "source": [
        "Now compile the model, call the fit method to do the training, and evaluate the loss and accuracy from the test set.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IXx_LX3SAlFs"
      },
      "source": [
        "# Visualizing the Convolutions and Pooling\n",
        "\n",
        "This code will show us the convolutions graphically. The print (test_labels[;100]) shows us the first 100 labels in the test set, and you can see that the ones at index 0, index 23 and index 28 are all the same value (9). They're all shoes. Let's take a look at the result of running the convolution on each, and you'll begin to see common features between them emerge. Now, when the DNN is training on that data, it's working with a lot less, and it's perhaps finding a commonality between shoes based on this convolution/pooling combination."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f-6nX4QsOku6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c5ab0d73-1d53-4681-8bc3-c8b149272c59"
      },
      "source": [
        "print(test_labels[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9 2 1 1 6 1 4 6 5 7 4 5 7 3 4 1 2 4 8 0 2 5 7 9 1 4 6 0 9 3 8 8 3 3 8 0 7\n",
            " 5 7 9 6 1 3 7 6 7 2 1 2 2 4 4 5 8 2 2 8 4 8 0 7 7 8 5 1 1 2 3 9 8 7 0 2 6\n",
            " 2 3 1 2 8 4 1 8 5 9 5 0 3 2 0 6 5 3 6 7 1 8 0 1 4 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9FGsHhv6JvDx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "b4acb404-e581-46e8-8a2e-b0a0a7bbd67c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "f, axarr = plt.subplots(3,4)\n",
        "FIRST_IMAGE=0\n",
        "SECOND_IMAGE=23\n",
        "THIRD_IMAGE=28\n",
        "CONVOLUTION_NUMBER = 2\n",
        "from tensorflow.keras import models\n",
        "layer_outputs = [layer.output for layer in model.layers]\n",
        "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
        "for x in range(0,4):\n",
        "  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[0,x].grid(False)\n",
        "  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[1,x].grid(False)\n",
        "  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[2,x].grid(False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 12 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyAfGUDGRbg-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "6c14dced-38c3-425f-8a63-586820ea7e91"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "f, axarr = plt.subplots(3,4)\n",
        "FIRST_IMAGE=0\n",
        "SECOND_IMAGE=23\n",
        "THIRD_IMAGE=28\n",
        "CONVOLUTION_NUMBER = 63\n",
        "from tensorflow.keras import models\n",
        "layer_outputs = [layer.output for layer in model.layers]\n",
        "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
        "for x in range(0,4):\n",
        "  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[0,x].grid(False)\n",
        "  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[1,x].grid(False)\n",
        "  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[2,x].grid(False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 12 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8KVPZqgHo5Ux"
      },
      "source": [
        "EXERCISES\n",
        "\n",
        "1. Try editing the convolutions. Change the 32s to either 16 or 64. What impact will this have on accuracy and/or training time.\n",
        "\n",
        "2. Remove the final Convolution. What impact will this have on accuracy or training time?\n",
        "\n",
        "3. How about adding more Convolutions? What impact do you think this will have? Experiment with it.\n",
        "\n",
        "4. Remove all Convolutions but the first. What impact do you think this will have? Experiment with it. \n",
        "\n",
        "5. In the previous lesson you implemented a callback to check on the loss function and to cancel training once it hit a certain amount. See if you can implement that here!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZpYRidBXpBPM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "629f28da-4642-4580-8b02-c53384da05c1"
      },
      "source": [
        "%%time\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1504 - accuracy: 0.9555\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0515 - accuracy: 0.9840\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0337 - accuracy: 0.9893\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0224 - accuracy: 0.9929\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0163 - accuracy: 0.9945\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0109 - accuracy: 0.9968\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0084 - accuracy: 0.9973\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0068 - accuracy: 0.9978\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0050 - accuracy: 0.9983\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0050 - accuracy: 0.9984\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0552 - accuracy: 0.9859\n",
            "0.9858999848365784\n",
            "CPU times: user 55.7 s, sys: 6.39 s, total: 1min 2s\n",
            "Wall time: 52.8 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ3MSXH8vGqH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "d8a1e02e-77f4-4ab3-9c82-74604c63e738"
      },
      "source": [
        "%%time\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2585 - accuracy: 0.9276\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1136 - accuracy: 0.9663\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0786 - accuracy: 0.9760\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0591 - accuracy: 0.9822\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0457 - accuracy: 0.9859\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0365 - accuracy: 0.9887\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0289 - accuracy: 0.9910\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0233 - accuracy: 0.9932\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0192 - accuracy: 0.9938\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0161 - accuracy: 0.9952\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0933 - accuracy: 0.9751\n",
            "0.9750999808311462\n",
            "CPU times: user 47.5 s, sys: 6.35 s, total: 53.8 s\n",
            "Wall time: 45.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUxDyMplvPgZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "8374da00-cc13-4b0b-c91f-d9892a3c0c60"
      },
      "source": [
        "%%time\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1372 - accuracy: 0.9594\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0467 - accuracy: 0.9855\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0299 - accuracy: 0.9904\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0170 - accuracy: 0.9948\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0130 - accuracy: 0.9958\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0092 - accuracy: 0.9971\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0075 - accuracy: 0.9973\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0046 - accuracy: 0.9985\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0057 - accuracy: 0.9981\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0040 - accuracy: 0.9987\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0543 - accuracy: 0.9870\n",
            "0.9869999885559082\n",
            "CPU times: user 55.2 s, sys: 6.74 s, total: 1min 1s\n",
            "Wall time: 52.9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRszLZK-vUIz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "4b7ee3e8-4fae-40ba-9459-6d8914c26163"
      },
      "source": [
        "%%time\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(8, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1905 - accuracy: 0.9436\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0667 - accuracy: 0.9796\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0444 - accuracy: 0.9864\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0321 - accuracy: 0.9894\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0234 - accuracy: 0.9926\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0176 - accuracy: 0.9943\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0124 - accuracy: 0.9962\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0114 - accuracy: 0.9964\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0087 - accuracy: 0.9971\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0064 - accuracy: 0.9980\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0611 - accuracy: 0.9851\n",
            "0.9850999712944031\n",
            "CPU times: user 55.1 s, sys: 6.29 s, total: 1min 1s\n",
            "Wall time: 51.9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlG4zHIgvWyJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "19fb9b37-7a09-47e0-9089-43c3ee75adf8"
      },
      "source": [
        "%%time\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1338 - accuracy: 0.9594\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0414 - accuracy: 0.9872\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0219 - accuracy: 0.9930\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0127 - accuracy: 0.9957\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0099 - accuracy: 0.9964\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0070 - accuracy: 0.9977\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0047 - accuracy: 0.9984\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0050 - accuracy: 0.9984\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0041 - accuracy: 0.9988\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0026 - accuracy: 0.9992\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0796 - accuracy: 0.9849\n",
            "0.9848999977111816\n",
            "CPU times: user 56.7 s, sys: 7.25 s, total: 1min 3s\n",
            "Wall time: 54.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAGzoXa1vgzv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}